<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Farzad" />

<meta name="date" content="2016-08-15" />

<title>Machine Learning Time-series Predicting Pipeline Guide</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="libs/highlight/default.css"
      type="text/css" />
<script src="libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Machine Learning Time-series Predicting Pipeline Guide</h1>
<h4 class="author"><em>Farzad</em></h4>
<h4 class="date"><em>15 August 2016</em></h4>

</div>


<div id="setup" class="section level2">
<h2>Setup</h2>
<p>For the rest of this guide, we will use our own time-series:</p>
<pre class="r"><code>library(&quot;xts&quot;)
library(&quot;mltsp&quot;)

n = 25
stamps &lt;- seq(from = as.Date(&quot;2010-01-01&quot;), to=as.Date(&quot;2010-01-25&quot;), by = &quot;day&quot;)
observed_data &lt;- xts(as.numeric(1:n) + runif(n), stamps)</code></pre>
<p>Then we train on the first 20 values.</p>
<pre class="r"><code>train_data &lt;- head(observed_data, 20)</code></pre>
<div id="note" class="section level4">
<h4>Note</h4>
<p><code>timeDate</code> is usually slow. Use <code>Date</code> or <code>POSIXct</code> whenever possible.</p>
</div>
</div>
<div id="the-pipeline" class="section level2">
<h2>The pipeline</h2>
<p>Every time-series prediction tool has three steps:</p>
<ul>
<li>Pre-processing: We will use a simple difference, with the order determined automatically (See <code>ndiffs</code> function in package <code>forecast</code> with ‘kpss’ measure).</li>
<li>Feature extraction: We will use lag windows.</li>
<li>Learner: We will use a simple linear model.</li>
</ul>
<p>We implement these using</p>
<pre class="r"><code>pp &lt;- list(list(&quot;diff&quot;, &quot;auto&quot;))
fx &lt;- function(x) cbind(x, lag_windows(x, p=1))
ln &lt;- SimpleLM
fcster &lt;- mltsp_forecaster(pp, fx, ln)</code></pre>
<p>Now <code>fcster</code> can be used to forecast the series:</p>
<pre class="r"><code>fcster(train_data, h=5)</code></pre>
<pre><code>##                [,1]
## 2010-01-21 21.68824
## 2010-01-22 22.54645
## 2010-01-23 23.62475
## 2010-01-24 24.59541
## 2010-01-25 25.61871</code></pre>
<p><img src="pipeline_guide_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div id="alternative-model-creation" class="section level3">
<h3>Alternative model creation</h3>
<p>A <code>forecast</code> compatible model:</p>
<pre class="r"><code>model &lt;- mltsp(train_data, pp, fx, ln)</code></pre>
<p>and applying forecast to that model:</p>
<pre class="r"><code>forecast(model, h=5)</code></pre>
<pre><code>##                [,1]
## 2010-01-21 21.68824
## 2010-01-22 22.54645
## 2010-01-23 23.62475
## 2010-01-24 24.59541
## 2010-01-25 25.61871</code></pre>
<p>One can simply reuse the same model with other data:</p>
<pre class="r"><code>model2 &lt;- mltsp(model, observed_data)</code></pre>
</div>
<div id="pre-processing" class="section level3">
<h3>Pre-processing</h3>
<p>Pre-processing parameter contains a <code>list</code> of pre-processing techniques to be applied consecutively. Each techniques itself is in form of <code>list(&quot;techninque name&quot;, param1, param2, ...)</code>.</p>
<p>Available techniques include:</p>
<ul>
<li><code>&quot;diff&quot;</code>: Differencing the time-series. Parameter can be an integer (the differencing order), “kpss”, “adf”, or “auto”.</li>
<li><code>&quot;boxcox&quot;</code>: Box-cox transform. Parameter can be a numeric lambda, or “auto”.</li>
<li><code>&quot;log&quot;</code>: Log transform. Only works if time-series is positive. No parameters.</li>
<li><code>&quot;log_abs&quot;</code>: Log transform on absolute value. Destroys values between -1 and 1. No parameters.</li>
<li><code>&quot;log+min&quot;</code>: Log transform on <code>(x - min(x) + 1)</code>. No parameters.</li>
</ul>
</div>
<div id="feature-extraction" class="section level3">
<h3>Feature Extraction</h3>
<p>The above could have been replaced with:</p>
<pre class="r"><code>fx &lt;- function(x) lag_windows(x, p=1, no_peeking = FALSE)</code></pre>
<p>Here, <code>no_peeking = FALSE</code> allows the first column to be the <em>observed</em> data (i.e. zero lags), which will serve as the training target in the learning function.</p>
<p>Other options are:</p>
<ul>
<li><code>lag_windows(x, p, P = 0, freq = 1, shift = 0, no_peeking = TRUE, pstr)</code></li>
<li><code>centered_lag_windows(x, p = 1, pstr=deparse(substitute(x)))</code></li>
<li><code>seasonal_lag_windows(x, P = 1, freq = frequency(x), width = 0, shift = 0, no_peeking = TRUE, pstr)</code></li>
</ul>
</div>
<div id="learner" class="section level3">
<h3>Learner</h3>
<p>The learner should take whatever comes out of feature extraction and create a model up on it. For example, using formulas if the name of the target column in feature extraction is known:</p>
<pre class="r"><code>ln &lt;- function(f) lm(x ~ ., f)</code></pre>
<p>Alternatively, one can use function that take a data.frame and consider the training target to be the first column, such as <code>SimpleLM</code>, or <code>svm</code> from package <code>e1071</code>.</p>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross-validation</h2>
<p>To compare different models, we use cross validation (CV).</p>
<p>For example, consider the above model versus the one without <em>first difference</em> pre-processing:</p>
<pre class="r"><code>fcster_nodiff &lt;- mltsp_forecaster(NULL, 
                                  function(x) lag_windows(x, p=3, no_peeking = TRUE),
                                  SimpleLM)</code></pre>
<p>CV using time-series slices are impelemted in <code>ts_crossval</code>. By default, <code>rmse</code> is used as the error measure. Parameters are: * <code>horizon</code>: Forecasting horizon to test the algorithm on * <code>initial_window</code>: How long should be the smallest window of data used for training. See documentation for more information.</p>
<p>Here, we have:</p>
<pre class="r"><code>ts_crossval(train_data, fcster, horizon = 5, initial_window = 10)</code></pre>
<pre><code>## [1] 2.694877</code></pre>
<pre class="r"><code>ts_crossval(train_data, fcster_nodiff, horizon = 5, initial_window = 10)</code></pre>
<pre><code>## [1] 13.06756</code></pre>
<p>The first one (i.e., using first difference) has a smaller CV error and is therefore better. Here is their out-of-sample forecast vs actual data: <img src="pipeline_guide_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div id="early-break-batches-and-parallel-processing" class="section level3">
<h3>Early break, batches and parallel processing</h3>
<p>When using <code>ts_crossval</code> you don’t have to fully cross-validate to see if something is not as good as what you expect. For example, if CV error has surpased a naive algorithm, one can simply terminate the cross-validation process. This can be done by setting</p>
<ul>
<li><code>break_err</code></li>
<li><code>break_batch_err</code></li>
<li><code>break_batch_size</code></li>
</ul>
<p>For example, in the above example, first model has an accumulative CV error = 2.70, and CV is applied to 6 slices:</p>
<pre class="r"><code>ts_crossval(train_data, fcster, horizon = 5, initial_window = 10, verbose = TRUE)</code></pre>
<pre><code>## [1] &quot;CV: 1 of 6 Err:  0.458098568928112&quot;
## [1] &quot;CV: 2 of 6 Err:  0.347115924471326&quot;
## [1] &quot;CV: 3 of 6 Err:  0.545312229495626&quot;
## [1] &quot;CV: 4 of 6 Err:  0.706615761722711&quot;
## [1] &quot;CV: 5 of 6 Err:  0.190170722457397&quot;
## [1] &quot;CV: 6 of 6 Err:  0.447563850135199&quot;</code></pre>
<pre><code>## [1] 2.694877</code></pre>
<p>So any model with CV error &gt; 2.70 is worse than it. Let <code>break_err = 2.7</code>:</p>
<pre class="r"><code>ts_crossval(train_data, fcster_nodiff, horizon = 5, initial_window = 10, break_err = 2.7,  verbose = TRUE)</code></pre>
<pre><code>## [1] &quot;CV: 1 of 6 Err:  1.41358548236245&quot;
## [1] &quot;CV: 2 of 6 Err:  2.11831197319849&quot;
## [1] &quot;CV: Total error break: Err = 3.53189745556094&quot;</code></pre>
<pre><code>## [1] 3.531897</code></pre>
<p>CV terminates at the second slice, saving CV time of 4 slices.</p>
<p>The idea behind having a <em>batch size</em> is parallel processing. Many functions, including <code>mclapply</code> return once they have completed all computation. If this is used with <code>break_err</code>, checking for the break threshold happens only after all computation is done, and there is no use in aborting CV progress if everything is already calculated.</p>
<p>Consequenty, <code>tscrossval</code> is designed to allow one to select how much is given to <code>mclapply</code> in each <code>batch</code>. Selecting a large number results in more efficient parallel processing, but more coarse break error checking. A small number will result in higher parallel processing overheard.</p>
<p>For example,</p>
<pre class="r"><code>library(&quot;parallel&quot;)
options(mc.cores=2)

system.time({
err &lt;- ts_crossval(train_data, fcster_nodiff, horizon = 5, initial_window = 10, plapply = mclapply,
            break_err = 3,  verbose = TRUE)
})</code></pre>
<pre><code>## Warning in ts_crossval(train_data, fcster_nodiff, horizon = 5,
## initial_window = 10, : Break error is given, but batch size is not.</code></pre>
<pre><code>## [1] &quot;CV: Total error break: Err = 13.0675619396307&quot;</code></pre>
<pre><code>##    user  system elapsed 
##   0.031   0.020   0.055</code></pre>
<pre class="r"><code>paste(&quot;Without batch-size Error was &quot;, err)</code></pre>
<pre><code>## [1] &quot;Without batch-size Error was  13.0675619396307&quot;</code></pre>
<p>Which is what we expect if all 6 cross-validations were done. Note that verbose doesn’t print its result when it is inside child processes, the fucntion warns that <em>bacth size</em> is not given.</p>
<p>In comparison, a batch of size two (to fill the two available cores):</p>
<pre class="r"><code>system.time({
err &lt;- ts_crossval(train_data, fcster_nodiff, horizon = 5, initial_window = 10, plapply = mclapply,
            break_err = 3,  break_batch_size = 2, verbose = TRUE)
})</code></pre>
<pre><code>## [1] &quot;CV: Total error break: Err = 3.53189745556094&quot;</code></pre>
<pre><code>##    user  system elapsed 
##   0.085   0.043   0.043</code></pre>
<pre class="r"><code>paste(&quot;Without batch-size, final error was &quot;, err)</code></pre>
<pre><code>## [1] &quot;Without batch-size, final error was  3.53189745556094&quot;</code></pre>
<p>This one also terminates sooner (compare <em>elapsed</em> values).</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
